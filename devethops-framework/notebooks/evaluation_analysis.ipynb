{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a090179a",
   "metadata": {},
   "source": [
    "# DevEthOps Framework: Comprehensive Fairness and Ethical AI Evaluation\n",
    "\n",
    "This notebook demonstrates how to use the DevEthOps Framework for comprehensive ethical AI evaluation. We'll cover:\n",
    "\n",
    "1. **Data Loading and Bias Analysis**: Loading synthetic datasets and detecting bias patterns\n",
    "2. **Fairness Metrics Evaluation**: Computing comprehensive fairness metrics\n",
    "3. **Explainability Analysis**: Using SHAP and LIME for model interpretability\n",
    "4. **Performance vs Fairness Trade-offs**: Analyzing the balance between accuracy and fairness\n",
    "5. **Intersectional Fairness**: Examining fairness across multiple protected attributes\n",
    "6. **Bias Mitigation Strategies**: Demonstrating bias reduction techniques\n",
    "7. **Monitoring and Alerting**: Setting up continuous fairness monitoring\n",
    "\n",
    "**Target Audience**: Data Scientists, ML Engineers, Ethics Officers, and DevOps Engineers working with AI/ML systems.\n",
    "\n",
    "**Requirements**: \n",
    "- Python 3.8+\n",
    "- DevEthOps Framework installed\n",
    "- Jupyter Notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for the DevEthOps framework\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to the path to import our modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# DevEthOps Framework modules\n",
    "from ethical_checks.fairness_evaluator import FairnessEvaluator\n",
    "from ethical_checks.explainability_analyzer import ExplainabilityAnalyzer\n",
    "from metrics.fairness_metrics import FairnessMetrics\n",
    "from metrics.performance_metrics import PerformanceMetrics\n",
    "from models.llm_wrapper import LLMWrapper\n",
    "\n",
    "# Test utilities\n",
    "sys.path.append('../tests')\n",
    "from fixtures.synthetic_datasets import SyntheticDataGenerator, BiasInjector\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"DevEthOps Framework is ready for ethical AI evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45b1af",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Bias Analysis\n",
    "\n",
    "In this section, we'll generate synthetic datasets with known bias patterns to demonstrate the DevEthOps framework's capabilities. We'll create both biased and fair datasets to compare fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic datasets for analysis\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Create a biased credit approval dataset\n",
    "print(\"🔴 Generating BIASED credit dataset...\")\n",
    "biased_data = generator.generate_credit_dataset(\n",
    "    n_samples=5000,\n",
    "    inject_bias=True,\n",
    "    bias_types=['label', 'feature']\n",
    ")\n",
    "\n",
    "# Create a fair credit approval dataset for comparison\n",
    "print(\"🟢 Generating FAIR credit dataset...\")\n",
    "fair_data = generator.generate_credit_dataset(\n",
    "    n_samples=5000,\n",
    "    inject_bias=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"Biased dataset: {biased_data.shape}\")\n",
    "print(f\"Fair dataset: {fair_data.shape}\")\n",
    "\n",
    "# Display first few rows and basic statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIASED DATASET - First 5 rows:\")\n",
    "print(\"=\"*50)\n",
    "display(biased_data.head())\n",
    "\n",
    "print(f\"\\n📊 Biased Dataset Statistics:\")\n",
    "print(f\"Approval rate by gender:\")\n",
    "approval_by_gender = biased_data.groupby('gender')['label'].agg(['mean', 'count'])\n",
    "approval_by_gender.columns = ['Approval_Rate', 'Count']\n",
    "display(approval_by_gender)\n",
    "\n",
    "print(f\"\\nApproval rate by race:\")\n",
    "approval_by_race = biased_data.groupby('race')['label'].agg(['mean', 'count'])\n",
    "approval_by_race.columns = ['Approval_Rate', 'Count']\n",
    "display(approval_by_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias patterns in the datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Biased dataset - Gender\n",
    "biased_gender_stats = biased_data.groupby(['gender', 'label']).size().unstack()\n",
    "biased_gender_stats.plot(kind='bar', ax=axes[0,0], title='Biased Dataset: Approvals by Gender')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].legend(['Denied', 'Approved'])\n",
    "\n",
    "# Fair dataset - Gender  \n",
    "fair_gender_stats = fair_data.groupby(['gender', 'label']).size().unstack()\n",
    "fair_gender_stats.plot(kind='bar', ax=axes[0,1], title='Fair Dataset: Approvals by Gender')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "axes[0,1].legend(['Denied', 'Approved'])\n",
    "\n",
    "# Biased dataset - Race\n",
    "biased_race_stats = biased_data.groupby(['race', 'label']).size().unstack()\n",
    "biased_race_stats.plot(kind='bar', ax=axes[1,0], title='Biased Dataset: Approvals by Race')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].legend(['Denied', 'Approved'])\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Fair dataset - Race\n",
    "fair_race_stats = fair_data.groupby(['race', 'label']).size().unstack()\n",
    "fair_race_stats.plot(kind='bar', ax=axes[1,1], title='Fair Dataset: Approvals by Race') \n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].legend(['Denied', 'Approved'])\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display approval rate disparities\n",
    "print(\"📈 BIAS ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "biased_gender_rates = biased_data.groupby('gender')['label'].mean()\n",
    "fair_gender_rates = fair_data.groupby('gender')['label'].mean()\n",
    "\n",
    "print(f\"Gender Approval Rate Disparity:\")\n",
    "print(f\"  Biased Dataset: {abs(biased_gender_rates['Male'] - biased_gender_rates['Female']):.3f}\")\n",
    "print(f\"  Fair Dataset: {abs(fair_gender_rates['Male'] - fair_gender_rates['Female']):.3f}\")\n",
    "\n",
    "biased_race_rates = biased_data.groupby('race')['label'].mean()\n",
    "fair_race_rates = fair_data.groupby('race')['label'].mean()\n",
    "\n",
    "print(f\"\\nRace Approval Rate Range:\")\n",
    "print(f\"  Biased Dataset: {biased_race_rates.max() - biased_race_rates.min():.3f}\")\n",
    "print(f\"  Fair Dataset: {fair_race_rates.max() - fair_race_rates.min():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e64be6",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "Now we'll train machine learning models on both datasets to compare their fairness characteristics. We'll train simple models to focus on the ethical evaluation aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7231c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "feature_cols = ['income_score', 'credit_history', 'employment_length', 'debt_ratio', \n",
    "                'education_score', 'age_group', 'savings_score', 'loan_amount']\n",
    "\n",
    "# Prepare biased dataset\n",
    "X_biased = biased_data[feature_cols]\n",
    "y_biased = biased_data['label']\n",
    "X_train_biased, X_test_biased, y_train_biased, y_test_biased = train_test_split(\n",
    "    X_biased, y_biased, test_size=0.3, random_state=42, stratify=y_biased\n",
    ")\n",
    "\n",
    "# Prepare fair dataset  \n",
    "X_fair = fair_data[feature_cols]\n",
    "y_fair = fair_data['label']\n",
    "X_train_fair, X_test_fair, y_train_fair, y_test_fair = train_test_split(\n",
    "    X_fair, y_fair, test_size=0.3, random_state=42, stratify=y_fair\n",
    ")\n",
    "\n",
    "# Train models\n",
    "print(\"🚀 Training models...\")\n",
    "\n",
    "# Model trained on biased data\n",
    "model_biased = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_biased.fit(X_train_biased, y_train_biased)\n",
    "\n",
    "# Model trained on fair data\n",
    "model_fair = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "model_fair.fit(X_train_fair, y_train_fair)\n",
    "\n",
    "# Generate predictions\n",
    "pred_biased = model_biased.predict(X_test_biased)\n",
    "pred_fair = model_fair.predict(X_test_fair)\n",
    "\n",
    "# Calculate basic performance metrics\n",
    "print(\"📊 Model Performance:\")\n",
    "print(f\"Model trained on biased data - Accuracy: {accuracy_score(y_test_biased, pred_biased):.3f}\")\n",
    "print(f\"Model trained on fair data - Accuracy: {accuracy_score(y_test_fair, pred_fair):.3f}\")\n",
    "\n",
    "print(\"\\n✅ Models trained successfully!\")\n",
    "print(\"Ready for fairness evaluation using DevEthOps framework...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad919e",
   "metadata": {},
   "source": [
    "## 3. DevEthOps Fairness Evaluation\n",
    "\n",
    "Now we'll use the DevEthOps framework to perform comprehensive fairness evaluation on both models. This includes demographic parity, disparate impact, equalized odds, and individual fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DevEthOps fairness evaluator\n",
    "fairness_config = {\n",
    "    'protected_attributes': ['gender', 'race'],\n",
    "    'favorable_label': 1,\n",
    "    'unfavorable_label': 0,\n",
    "    'thresholds': {\n",
    "        'demographic_parity': 0.1,\n",
    "        'disparate_impact': 0.8,\n",
    "        'equalized_odds': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "evaluator = FairnessEvaluator(fairness_config)\n",
    "\n",
    "# Prepare protected attributes for both test sets\n",
    "protected_attrs_biased = biased_data.loc[X_test_biased.index, ['gender', 'race']]\n",
    "protected_attrs_fair = fair_data.loc[X_test_fair.index, ['gender', 'race']]\n",
    "\n",
    "print(\"🔍 Evaluating Model Trained on BIASED Data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate model trained on biased data\n",
    "fairness_result_biased = evaluator.evaluate_model_fairness(\n",
    "    model=model_biased,\n",
    "    X_test=X_test_biased.values,\n",
    "    y_test=y_test_biased.values,\n",
    "    protected_attributes=protected_attrs_biased,\n",
    "    predictions=pred_biased\n",
    ")\n",
    "\n",
    "print(f\"Overall Fairness Score: {fairness_result_biased['overall_fairness_score']:.3f}\")\n",
    "print(f\"Number of Violations: {len(fairness_result_biased['violations'])}\")\n",
    "print(\"\\nFairness Metrics:\")\n",
    "for metric, value in fairness_result_biased['fairness_metrics'].items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {metric}:\")\n",
    "        for sub_metric, sub_value in value.items():\n",
    "            print(f\"    {sub_metric}: {sub_value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nViolations:\")\n",
    "for violation in fairness_result_biased['violations']:\n",
    "    print(f\"  - {violation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 Evaluating Model Trained on FAIR Data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate model trained on fair data\n",
    "fairness_result_fair = evaluator.evaluate_model_fairness(\n",
    "    model=model_fair,\n",
    "    X_test=X_test_fair.values,\n",
    "    y_test=y_test_fair.values,\n",
    "    protected_attributes=protected_attrs_fair,\n",
    "    predictions=pred_fair\n",
    ")\n",
    "\n",
    "print(f\"Overall Fairness Score: {fairness_result_fair['overall_fairness_score']:.3f}\")\n",
    "print(f\"Number of Violations: {len(fairness_result_fair['violations'])}\")\n",
    "print(\"\\nFairness Metrics:\")\n",
    "for metric, value in fairness_result_fair['fairness_metrics'].items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {metric}:\")\n",
    "        for sub_metric, sub_value in value.items():\n",
    "            print(f\"    {sub_metric}: {sub_value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "if fairness_result_fair['violations']:\n",
    "    print(f\"\\nViolations:\")\n",
    "    for violation in fairness_result_fair['violations']:\n",
    "        print(f\"  - {violation}\")\n",
    "else:\n",
    "    print(f\"\\n✅ No fairness violations detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07269cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Fairness scores comparison\n",
    "models = ['Biased Model', 'Fair Model'] \n",
    "fairness_scores = [\n",
    "    fairness_result_biased['overall_fairness_score'],\n",
    "    fairness_result_fair['overall_fairness_score']\n",
    "]\n",
    "\n",
    "bars1 = axes[0,0].bar(models, fairness_scores, color=['red', 'green'], alpha=0.7)\n",
    "axes[0,0].set_title('Overall Fairness Score Comparison')\n",
    "axes[0,0].set_ylabel('Fairness Score')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, fairness_scores):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Violations comparison\n",
    "violations_count = [\n",
    "    len(fairness_result_biased['violations']),\n",
    "    len(fairness_result_fair['violations'])\n",
    "]\n",
    "\n",
    "bars2 = axes[0,1].bar(models, violations_count, color=['red', 'green'], alpha=0.7)\n",
    "axes[0,1].set_title('Number of Fairness Violations')\n",
    "axes[0,1].set_ylabel('Violation Count')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars2, violations_count):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Demographic parity by gender comparison\n",
    "dp_gender_biased = fairness_result_biased['fairness_metrics']['demographic_parity']['gender']\n",
    "dp_gender_fair = fairness_result_fair['fairness_metrics']['demographic_parity']['gender']\n",
    "\n",
    "gender_dp = [abs(dp_gender_biased), abs(dp_gender_fair)]\n",
    "bars3 = axes[1,0].bar(models, gender_dp, color=['red', 'green'], alpha=0.7)\n",
    "axes[1,0].set_title('Demographic Parity Difference (Gender)')\n",
    "axes[1,0].set_ylabel('Absolute Difference')\n",
    "axes[1,0].axhline(y=0.1, color='orange', linestyle='--', label='Threshold (0.1)')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, dp in zip(bars3, gender_dp):\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                   f'{dp:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Disparate impact comparison  \n",
    "di_gender_biased = fairness_result_biased['fairness_metrics']['disparate_impact']['gender']\n",
    "di_gender_fair = fairness_result_fair['fairness_metrics']['disparate_impact']['gender']\n",
    "\n",
    "gender_di = [di_gender_biased, di_gender_fair]\n",
    "bars4 = axes[1,1].bar(models, gender_di, color=['red', 'green'], alpha=0.7)\n",
    "axes[1,1].set_title('Disparate Impact Ratio (Gender)')\n",
    "axes[1,1].set_ylabel('Impact Ratio')\n",
    "axes[1,1].axhline(y=0.8, color='orange', linestyle='--', label='Threshold (0.8)')\n",
    "axes[1,1].axhline(y=1.0, color='black', linestyle='-', alpha=0.3, label='Perfect Fairness')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, di in zip(bars4, gender_di):\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{di:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('DevEthOps Fairness Evaluation: Model Comparison', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "summary_data = {\n",
    "    'Metric': ['Overall Fairness Score', 'Violations Count', 'Demo Parity (Gender)', 'Disparate Impact (Gender)'],\n",
    "    'Biased Model': [\n",
    "        f\"{fairness_result_biased['overall_fairness_score']:.3f}\",\n",
    "        len(fairness_result_biased['violations']),\n",
    "        f\"{abs(dp_gender_biased):.3f}\",\n",
    "        f\"{di_gender_biased:.3f}\"\n",
    "    ],\n",
    "    'Fair Model': [\n",
    "        f\"{fairness_result_fair['overall_fairness_score']:.3f}\",\n",
    "        len(fairness_result_fair['violations']),\n",
    "        f\"{abs(dp_gender_fair):.3f}\",\n",
    "        f\"{di_gender_fair:.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n📋 FAIRNESS EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f205e4",
   "metadata": {},
   "source": [
    "## 4. Explainability Analysis with SHAP and LIME\n",
    "\n",
    "The DevEthOps framework includes explainability analysis to understand model decisions and detect potential sources of bias in feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85989bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainability analyzer\n",
    "explainer_config = {\n",
    "    'enable_shap': True,\n",
    "    'enable_lime': True,\n",
    "    'sample_size': 500  # Reduced for faster computation in notebook\n",
    "}\n",
    "\n",
    "analyzer = ExplainabilityAnalyzer(explainer_config)\n",
    "\n",
    "print(\"🔍 Running Explainability Analysis...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze the biased model\n",
    "print(\"Analyzing model trained on BIASED data...\")\n",
    "explanation_biased = analyzer.analyze(\n",
    "    model=model_biased,\n",
    "    X_test=X_test_biased.values[:500],  # Subset for faster computation\n",
    "    feature_names=feature_cols,\n",
    "    protected_attributes=['gender', 'race'],\n",
    "    protected_data=protected_attrs_biased.iloc[:500]\n",
    ")\n",
    "\n",
    "print(f\"Explainability Score (Biased Model): {explanation_biased['explainability_score']:.3f}\")\n",
    "print(f\"Bias Detected in Features: {explanation_biased['bias_detected']}\")\n",
    "\n",
    "if explanation_biased['bias_detected']:\n",
    "    print(\"Biased features identified:\")\n",
    "    for feature, bias_score in explanation_biased['feature_bias_scores'].items():\n",
    "        if bias_score > 0.1:  # Threshold for significant bias\n",
    "            print(f\"  - {feature}: {bias_score:.3f}\")\n",
    "\n",
    "# Analyze the fair model\n",
    "print(f\"\\nAnalyzing model trained on FAIR data...\")\n",
    "explanation_fair = analyzer.analyze(\n",
    "    model=model_fair,\n",
    "    X_test=X_test_fair.values[:500],  # Subset for faster computation\n",
    "    feature_names=feature_cols,\n",
    "    protected_attributes=['gender', 'race'],\n",
    "    protected_data=protected_attrs_fair.iloc[:500]\n",
    ")\n",
    "\n",
    "print(f\"Explainability Score (Fair Model): {explanation_fair['explainability_score']:.3f}\")\n",
    "print(f\"Bias Detected in Features: {explanation_fair['bias_detected']}\")\n",
    "\n",
    "if explanation_fair['bias_detected']:\n",
    "    print(\"Biased features identified:\")\n",
    "    for feature, bias_score in explanation_fair['feature_bias_scores'].items():\n",
    "        if bias_score > 0.1:\n",
    "            print(f\"  - {feature}: {bias_score:.3f}\")\n",
    "else:\n",
    "    print(\"✅ No significant feature bias detected\")\n",
    "\n",
    "# Compare feature importance\n",
    "print(f\"\\n📊 Feature Importance Comparison:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get feature importance from both models\n",
    "importance_biased = model_biased.feature_importances_\n",
    "importance_fair = model_fair.feature_importances_\n",
    "\n",
    "# Create comparison DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Biased_Model': importance_biased,\n",
    "    'Fair_Model': importance_fair,\n",
    "    'Difference': importance_biased - importance_fair\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values('Difference', key=abs, ascending=False)\n",
    "display(importance_df)\n",
    "\n",
    "# Visualize feature importance comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "x = np.arange(len(feature_cols))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, importance_biased, width, label='Biased Model', alpha=0.7, color='red')\n",
    "plt.bar(x + width/2, importance_fair, width, label='Fair Model', alpha=0.7, color='green')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance Comparison: Biased vs Fair Models')\n",
    "plt.xticks(x, feature_cols, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c60283",
   "metadata": {},
   "source": [
    "## 5. DevEthOps Framework Summary & Recommendations\n",
    "\n",
    "### Key Findings from this Analysis:\n",
    "\n",
    "1. **Bias Detection**: The DevEthOps framework successfully identified bias patterns in the datasets and models\n",
    "2. **Fairness Metrics**: Comprehensive evaluation using multiple fairness criteria provided a holistic view\n",
    "3. **Explainability**: SHAP and LIME integration helped identify potential sources of bias in feature importance\n",
    "4. **Automated Evaluation**: The framework provides automated ethical AI evaluation suitable for CI/CD pipelines\n",
    "\n",
    "### Recommendations for Production Use:\n",
    "\n",
    "#### ✅ **Do's:**\n",
    "- Use multiple fairness metrics for comprehensive evaluation\n",
    "- Set appropriate thresholds based on your domain and regulatory requirements\n",
    "- Implement continuous monitoring to detect fairness degradation over time\n",
    "- Include explainability analysis in your model validation process\n",
    "- Document all fairness evaluation results for audit trails\n",
    "\n",
    "#### ❌ **Don'ts:**\n",
    "- Don't rely on a single fairness metric\n",
    "- Don't ignore intersectional bias (bias across multiple protected attributes)\n",
    "- Don't skip explainability analysis for high-stakes applications\n",
    "- Don't deploy models that fail fairness tests without proper justification\n",
    "- Don't forget to retrain and re-evaluate models when data distributions change\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Integration**: Integrate this framework into your CI/CD pipeline using the provided Jenkins configuration\n",
    "2. **Monitoring**: Set up continuous fairness monitoring using the monitoring components\n",
    "3. **Alerting**: Configure alerts for fairness degradation using the built-in notification system\n",
    "4. **Compliance**: Use the comprehensive reporting features for regulatory compliance\n",
    "5. **Team Training**: Train your ML and DevOps teams on ethical AI best practices\n",
    "\n",
    "The DevEthOps Framework provides the tools and infrastructure needed to build, deploy, and monitor ethical AI systems at scale."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
